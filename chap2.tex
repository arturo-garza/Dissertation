\chapter{Background}\label{ch:chapter2}

Machine translation has been a field of interest since the beginning of computers.

Before neural network language models became widely used in machine translation, statistical machine translation using \textit{n-gram} models was the state of the art technique. During this period, systems achieved good translations \textcolor{red}{How good}.

Statistical machine translation has problems to generalise words that are not widely seen in the data it trains on as the probabilities it uses to predict the translation is based on the counts of the time a certain word has occurred alone and its co-occurrences with other words. To reduce this problem, smoothing techniques were introduced. 

Neural Networks for Machine Translation is a method first used by~\citet*{kalchbrenner13emnlp}, who proposed using convolutional \textit{n-gram} models to extract fixed-length vector of a source sentence \citep{DBLP:journals/corr/ChungGCB14}. Afterwards, the authors [\citealp{DBLP:journals/corr/SutskeverVL14,DBLP:journals/corr/ChoMGBSB14}] used an Recurrent Neural Network encoder and decoder architecture. The three mentioned works were the pioneers in the use of different types of neural networks at their core. They all have in common that as their core, they implement the now widely used Encoder Decoder Architecture.

Since their first appearance, Neural Network Language Models have increasingly proven great contribution to the field as they can achieve very competitive translations when training them with a large quantity of data for language pairs such as English-French ~\citep*{DBLP:journals/corr/ZophYMK16}. This is because Neural Network Language Models are useful to model conditional probability distributions with several inputs \citep{DBLP:journals/corr/abs-1709-07809}. 

One of the questions that aroused when neural networks started to be used was how should words be represented to feed them into the neural network. The answer was presented by \citet*{DBLP:journals/corr/abs-1301-3781} and later improved in a subsequent paper \citep{DBLP:journals/corr/MikolovSCCD13}. They proposed representing words as a continuous vector representation. The proposed vector representation, provided state-of-the-art performance for measuring syntactic and semantic word similarities \citep{DBLP:journals/corr/abs-1301-3781}.

Similar to previous Statistical Machine Translation language models, Neural Network models tend to have problems to translate rare or unseen word.

This problem is more evide

It soon became clear that to obtain better translation quality with less data, that we needed 

 

As basic unit cells of modern Recurrent Neural Network Language Models there have been two proposals which are widely used nowadays as cells of the RNN. First, the Long Short Term Memory (LSTM) unit cell proposed by \citet{hochreiter1997long}. Its purpose is to alleviate the problem of vanishing or exploding gradients by . Second, the Gated Recurrent Units proposed by \citet*{DBLP:journals/corr/ChoMGBSB14} aims to .

In this work we use Gated Recurrent Units to conform with the Nematus Framework implementation. These units are defined as:

\textcolor{red}{Insert definition of GRU}
\citep{DBLP:journals/corr/ChungGCB14}

Having defined the general architecture of an Attention Based Recurrent Neural Network, here are some of the works that have been presented to alleviate the poor generalisation when in low resource conditions. The work presented by \citet*{DBLP:journals/corr/JeanCMB14}, tries to reduce the amount of unknown words by allowing the system to look up words in a dictionary. A disadvantage of this approach is that since we cannot have all the possible words that can happen in new data in a dictionary, then a way to handle the words that the system did not find needs to be put in place. This is exactly what the work done by \citet*{DBLP:journals/corr/GulcehreANZB16}, \citet*{DBLP:journals/corr/LuongSLVZ14} and \citet*{DBLP:journals/corr/GuLLL16} aim to solve.
When a word is unknown, their approach simply replaces them with UNK (unknown) tokens. The process is carried out in two stages, during the encoding process, words that are not known are replaced with UNK tokens. Then during the decoding stage, the UNK tokens are replaced back by copying the original word in the source text to the target one. While this is a sufficiently good approach for proper names like company names as the do not change from one language to the other, it is has encountered problems as it has been observed that the UNKs are replaced by incorrect words in the target text. While this approach was a good start to tackle the rare-word problem, it has no one word to one word equivalence and it is unnecessarily complex to have to train a separate dictionary. On a similar path to the work presented by \citet{DBLP:journals/corr/JeanCMB14}, \citet*{DBLP:journals/corr/MiWI16} presented a method that improves translation by narrowing down the vocabulary to a sentence or batch scope vocabulary size. To generate the reduced size vocabularies, \citet{DBLP:journals/corr/MiWI16} use a classical Statistical Machine Translation (SMT) system to build a word-to-word and a phrase-to-phrase translation models. This vocabulary reduction decreases the amount of UNK words by allowing the system to explore a larger vocabulary \citep{DBLP:journals/corr/abs-1710-01329}. While this improves the BLEU score ~\citep{Papineni02bleu:a} from their baseline by one point \citep{DBLP:journals/corr/MiWI16}. This method has as a drawback that it requires a separate model to be trained and this model still requires a large amount of data to achieve a good translation quality. As expected, the previous approaches do yield improvement in the translation score, nevertheless, they only address unknown words.

A second approach to this problem was followed by \citet*{DBLP:journals/corr/SennrichHB15} and \citet*{DBLP:journals/corr/ArthurNN16}. The approach presented by \citet{DBLP:journals/corr/SennrichHB15} is language independent, simple and efficient. It has as drawback that the segmentations it performs are not linguistically motivated which sometimes yield non optimal splits and that it still struggles with low count events. The approach presented by \citet{DBLP:journals/corr/ArthurNN16} attempts to address the rare word problem by gaining more information from the data. They learn information from the data by simply incorporating discrete, probabilistic (count based) lexicons as an additional source of information in the Neural Machine Translation system.  Their results do yield improvements in contrast to previous work. However, their proposed lexicon must be trained separately in an Stochastic Machine Translation (SMT) system, and its parameters can be difficult to handle in GPU memory \citep{DBLP:journals/corr/abs-1710-01329}. With this approach, \citet{DBLP:journals/corr/ArthurNN16} achieved an increase in BLEU of 2.0-2.3 points and 0.13-0.44 points in the NIST. Additionally, they observed qualitative improvements in the translations of content words. Even though these results are promising, the extra training step makes their approach too complex to train. Their work is the most similar to the one presented by \citet{DBLP:journals/corr/abs-1710-01329}.

A powerful idea is Transfer Learning, a technique published by~\citet*{DBLP:journals/corr/ZophYMK16}. This technique consists in using knowledge from a high-resource language pairs such as French-English to better translate low-resource language-pairs. The idea is that with the training step carried out on the high-resource language we improve the performance the system achieves to  the related task, typically reducing the amount of data required ~\citep*{DBLP:journals/corr/ZophYMK16}.

An interesting discovery made by \citet{DBLP:journals/corr/ZophYMK16} is that if the network is trained with a high-resource language pair that is related with the low-resource one, then the system obtained better performance.
Inspired by this technique, \citet*{DBLP:journals/corr/abs-1710-04087} proposed initialising with monolingual data. This approach is simpler than the one presented by \citet{DBLP:journals/corr/ZophYMK16} as it does not require a parallel corpus. In this theses we

Character level



%A second approach to this problem was followed by \citet*{DBLP:journals/corr/SennrichHB15} and \citet*{DBLP:journals/corr/ArthurNN16}. The approach presented by \citet{DBLP:journals/corr/SennrichHB15} is language independent, simple and efficient. It has as drawback that the segmentations it performs are not linguistically motivated which sometimes yield non optimal splits and that it still struggles with low count events. The approach presented by \citet{DBLP:journals/corr/ArthurNN16} attempts to address the rare word problem by gaining more information from the data. They learn information from the data by simply incorporating discrete, probabilistic (count based) lexicons as an additional source of information in the Neural Machine Translation system.  Their results do yield improvements in contrast to previous work. However, their proposed lexicon must be trained separately in an Stochastic Machine Translation (SMT) system, and its parameters can be difficult to handle in GPU memory \citep{DBLP:journals/corr/abs-1710-01329}. With this approach, \citet{DBLP:journals/corr/ArthurNN16} achieved an increase in BLEU of 2.0-2.3 points and 0.13-0.44 points in the NIST. Additionally, they observed qualitative improvements in the translations of content words. Even though these results are promising, the extra training step makes their approach too complex to train. Their work is the most similar to the one presented by \citet{DBLP:journals/corr/abs-1710-01329}. This is why we believe the simple Feed Forward Neural network that constitutes the Lexical model is a better approach as it does not need a separate training.